{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428dea08-556b-4cc7-ae3d-4aad50df29da",
   "metadata": {},
   "source": [
    "<img src=\"https://elearning.unidata.ucar.edu/dataeLearning/Cybertraining/analysis/media/NSF-Unidata_lockup_vertical_2023.png\" width=\"150px\" align=\"right\">\n",
    "\n",
    "# Machine Learning Analysis in the Earth Systems Sciences\n",
    "\n",
    "In this module, things happen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b54474a",
   "metadata": {},
   "source": [
    "## Optimize a precipitation classification machine learning model\n",
    "Sam and the research team made their first supervised machine learning classification model, but they need help taking their proof-of-concept model to the next level. Play the video below to learn more about how you will take on Sam's task.\n",
    "\n",
    "*Video opens in a new tab.*\n",
    "\n",
    "`<video>`\n",
    "https://docs.google.com/document/d/1utswbyDlktW17ZgoRDVAmRvCj_Q_Pic4w6B28OPJv58/edit?usp=sharing\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "<p class=\"admonition-title\" style=\"font-weight:bold\">What is a machine learning engineer?</p>\n",
    "    <p>Explanation to come</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce52aa5-bf0b-4925-aba1-f79d5e067f46",
   "metadata": {},
   "source": [
    "Let's begin with a review of the <b>problem framing</b> that Sam and the team laid out previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929e7ca-fc41-4eee-9e75-d11c950e1856",
   "metadata": {},
   "source": [
    "## Part 1: Problem Framing\n",
    "\n",
    "<center><a href=\"https://elearning.unidata.ucar.edu/dataeLearning/Cybertraining/analysis/media/RevisedFull_NOPROCESS_.png\" target=\"blank\"><img src=\"https://elearning.unidata.ucar.edu/dataeLearning/Cybertraining/analysis/media/RevisedFull_NOPROCESS_.png\"></a><br><i>Click to enlarge</i></center>\n",
    "<br><br>\n",
    "The first step in developing a machine learning model includes problem framing, which Sam and the team have already completed based on the task from their supervisor. They addressed the following questions.\n",
    "\n",
    "#### Problem framing questions\n",
    "As a part of the problem framing step, we answer a series of questions to ensure we're creating the best solution for this scenario. These questions were addressed in the previous module, which you may review below. \n",
    "\n",
    "***Does a simpler solution exist?***\n",
    "\n",
    "&emsp;Predicting the type of precipitation during a winter weather event relies on many different co-varying variables, including differences in temperature and humidity between the surface and clouds and winds, to name a few. A simple conditional statement may not be the most effective tool. \n",
    "\n",
    "***Can machine learning requirements be met?***\n",
    "\n",
    "&emsp;Sam will use a weather forecasting model in combination with citizen scientist precipitation type reports during winter weather events. They will have thousands of data records to use in development. \n",
    "\n",
    "***Which scientific question should be answered?***\n",
    "\n",
    "&emsp;\"Will the precipitation at a certain location and time be rain or snow?\"\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d012dc1-961c-4eb2-91ac-b72393d0e17a",
   "metadata": {},
   "source": [
    "## Part 2: Data Handling\n",
    "Data handling is the multi-step process for preparing data for model development. During this phase, data are gathered, examined, and split into three groups for model development and evaluation. \n",
    "\n",
    "<center><a href=\"https://elearning.unidata.ucar.edu/dataeLearning/Cybertraining/analysis/media/RevisedFull_NOPROCESS_.png\" target=\"blank\"><img src=\"https://elearning.unidata.ucar.edu/dataeLearning/Cybertraining/analysis/media/RevisedFull_NOPROCESS_.png\"></a><br><i>Click to enlarge</i></center>\n",
    "<br><br>\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aab0d6-8ed8-4452-84fb-564289d6a474",
   "metadata": {},
   "source": [
    "### Part 2a: Locate Data of Interest\n",
    "\n",
    "Sam's team has already gathered the data to use in model development and shared it with you to review. They combined output from a numerical atmospheric model (<a href=\"https://rapidrefresh.noaa.gov/hrrr/\" target=\"blank\">NOAA HRRR</a>) with citizen science precipitation type reports from <a href=\"https://mping.nssl.noaa.gov/\" target=\"blank\">mPing</a> such that every precipitation report corresponds to a series of environmental variables at the same time and location. These data are open-access so they did not need to gain special permission for use. They then stored the new, combined dataset in a single file on a THREDDS Data Server. \n",
    "\n",
    "They also want to make this new dataset available for other scientists and researchers (like you!) to use and build upon their progress, so they have made the data available to everyone following the FAIR data principles. \n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "<p class=\"admonition-title\" style=\"font-weight:bold\">FAIR data principles</p>\n",
    "    <p><a href=\"https://www.go-fair.org/fair-principles/\" target=\"blank\">FAIR data principles</a> ensure that data are Findable, Accessible, Interoperable, and Reusable by the scientific community. Following FAIR data principles helps ensure that research is transparent, reusable, and contributes to the peer-review process that keeps science reliable and open to improvement.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c556939-ab71-48c4-b395-cd15e346b0a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "To clearly document the source and nature of the data, they have created the following metadata document. Review this information before starting the next step: exploring the data. \n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "#### Metadata Document for Preciptation Type Classification Data\n",
    "\n",
    "##### General Information\n",
    "\n",
    "Dataset Name: \n",
    "\n",
    "Description: \n",
    "\n",
    "Date Range: \n",
    "\n",
    "Geographic Coverage: Continental United States \n",
    "\n",
    "Data Frequency: \n",
    "\n",
    "Last Updated: \n",
    "\n",
    "##### Data Structure\n",
    "\n",
    "File Format: .parquet\n",
    "\n",
    "Number of Records: \n",
    "\n",
    "Columns (features):\n",
    "\n",
    "- TEMP_C_0_m: Air temperature (°C) at 0 meters above ground level.\n",
    "- T_DEWPOINT_C_0_m: Dewpoint (°C) at 0 meters above ground level.\n",
    "- PRES_Pa_0_m: Environmental pressure (Pa) at 0 meters above ground level.\n",
    "- UGRD_m/s_0_m: U-component (west to east) of wind speed (m/s) at 0 meters above ground level.\n",
    "- VGRD_m/s_0_m: V-component (south to north) of wind speed (m/s) at 0 meters above ground level.\n",
    "- ptype: Precipitation type reported to mPing (\"rain\" or \"snow\")\n",
    "\n",
    "##### Data Quality\n",
    "\n",
    "Missing Data: There is no missing data in this dataset\n",
    "\n",
    "Outlier Handling:  No outlier handling was done\n",
    "\n",
    "##### Data Provenance\n",
    "\n",
    "Sources:\n",
    "\n",
    "</div>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c629fa-ac2a-48e5-9ef7-944857c35b9b",
   "metadata": {},
   "source": [
    "### Part 2b: Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603467a4-4935-4229-9a18-fe1d488e6349",
   "metadata": {},
   "source": [
    "Now it's your turn to explore the data that the team has prepared. Before starting an analysis of any kind, it's important to familiarize yourself with the data before you use it. This way, you can identify any issues or limitations in the dataset before you start generating statistics or transforming the data. In this step, you will take a closer look at the input and target features with a few plots.\n",
    "\n",
    "First, let's read the data into this workspace. To begin, we must import several Python packages, including all the tools for reading the data from the THREDDS Data Server and opening it in this workspace.\n",
    "\n",
    "> **Instructions**\n",
    "> \n",
    "> Execute the  cell below.\n",
    "> \n",
    "> *This may take a moment to complete.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79aae53e-37ef-45f6-9049-3ab1a88d11d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Location of the data on the THREDDS data server (will update with final dataset)\n",
    "file_path = '../ptype_data/ptype_noise_5000.parquet'\n",
    "\n",
    "# Read data into this workspace\n",
    "df = pd.read_parquet(file_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39db53f7-c843-407a-8018-ebbc78ed867d",
   "metadata": {},
   "source": [
    "#### Explore target features\n",
    "Target features are the features we predict using a machine learning model. Since this is a classification task, the target features are the classes \"rain\" and \"snow.\"\n",
    "\n",
    "Let's first confirm the total number of records and the proportion of rain and snow events. \n",
    "\n",
    "> **Instructions**\n",
    "> \n",
    "> Execute the cell below to determine the total number of records (length, `len()`), and the number of rain and snow observations (`value_counts()`) in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c75625-1116-4939-a521-e6d33f68b34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in dataset: 5000\n",
      "ptype\n",
      "snow    3222\n",
      "rain    1778\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Total records in dataset:\", len(df))\n",
    "print(df[\"ptype\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152a624-cd05-40c1-b950-97f297623616",
   "metadata": {},
   "source": [
    "#### Explore input features\n",
    "Input features are the variables that the model uses to predict the target features. In this case, the input features are the environmental variables from the HRRR numerical atmospheric model. \n",
    "\n",
    "As we explore the input features, we examine the following characteristics: \n",
    "\n",
    "- Distribution of values\n",
    "- Unusual values or outliers\n",
    "- Correlation among variables\n",
    "\n",
    "We'll start by visualizating the graphical distribution of values as histograms. In the plotting widget below, you can choose to view all data, or visualize the differences in distribution by precipitation type. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e9afe13-a335-4036-89c1-15ef552d0521",
   "metadata": {},
   "source": [
    "(could possibly cut this, save this for the end)\n",
    "plotting widget\n",
    "    two dropdowns\n",
    "     1. Variable\n",
    "            each model variable\n",
    "     2. Data \n",
    "            all records (one color #06887f)\n",
    "            rain only (one color #77aadd)\n",
    "            snow only (one color #ee8866)\n",
    "            rain and snow (rain #77aadd snow #ee8866, overlaid, transparency) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be8bd093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Module2_tech import HistogramWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97561ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a5e88b75b14999b1be926a695da955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Column:', options=('TEMP_C_0_m', 'TEMP_C_1000_m', 'TEMP_C_5000_m', 'T_DEW…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widget = HistogramWidget(df)\n",
    "widget.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac14f352-d8a8-4267-b846-8c406d3dad79",
   "metadata": {},
   "source": [
    "We can also supplement these graphical representations of spread with a summary statistics table. Examine the statistics to locate any unusual values, or those that do not seem to be physically plausible. \n",
    "\n",
    "> **Instructions**\n",
    ">\n",
    "> Execute the cell below to generate a summary statistics table of the input features.\n",
    ">\n",
    "> The table includes the following statistics.\n",
    ">\n",
    "> | label | definition |\n",
    "> |----|----|\n",
    "> | count  | number of records |\n",
    "> | mean | average |\n",
    "> | std    | standard deviation   |\n",
    "> | min | minimum value |\n",
    "> | 25%, 50%, 75% | 25th, 50th, and 75th percentile of the distribution, respectively |\n",
    "> | max | maximum value |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74847a0f-53db-4fa1-8d17-d8960a399500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d31d2-7095-422f-b7b0-05213a15fd5d",
   "metadata": {},
   "source": [
    "Next we'll compare the input features more directly by comparing all records in a grid of plots. In these comparison grids, the scatter plots display the input features a given time. For example, the temperature at 0 m on the x-axis and the dewpoint at 0 m on the y-axis. The scatter plot markers denote the precipitation type, `x` for rain and `y` for snow. Scatter plots that show distinct clustering of precipitation types demonstrate that the input variables may be better predictors of rain versus snow. Where rain and snow markers are uniformly distributed, the input features show reduced skill in differentiating rain and snow. \n",
    "\n",
    "`graphical demonstration`\n",
    "\n",
    "The comparison plot grid displays histograms where the x- and y-axes are the same station. These are the same histograms that you plotted previously, displaying the distribution of all records by precipitation type.\n",
    "\n",
    "> **Instructions**\n",
    "> \n",
    "> Execute the cell below to generate the comparison plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb98013-27a4-4510-a61e-595c621b31f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'rain': '#77aadd', 'snow': '#ee8866'}\n",
    "markers = {'rain': '^', 'snow': 'o'} \n",
    "sns.pairplot(df[['TEMP_C_0_m', 'T_DEWPOINT_C_0_m', 'UGRD_m/s_0_m', 'VGRD_m/s_0_m', 'PRES_Pa_0_m', 'ptype' ]].sample(2000), hue='ptype', palette=colors, markers=markers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93507f6b-f993-44b3-bf0d-2fe9604004c8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<p class=\"admonition-title\" style=\"font-weight:bold\">Exercise 2b</p>\n",
    "    <p>Open your <b>Machine Learning Model Handbook</b> to Exercise 2b. Then describe your exploratory data analysis of any target and input features of note. Include the following:\n",
    "    <ul>\n",
    "        <li>How many rain and snow records are in the dataset?</li>\n",
    "        <li>Do the distributions of values make sense for the physical world?</li>\n",
    "        <li>Are there any unexpected values?</li>\n",
    "        <li>Which input features may be the strongest predictors of rain vs snow?</li>\n",
    "        <li>Include any <i>important</i> plots to illustrate your conclusions. Limit yourself to 5 plots. <br><i>To copy a plot image, hold shift, right click on the image, then select Copy.</i></li>\n",
    "    </ul>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e60f9e-b7f7-4f52-be1f-392d0d9e6b88",
   "metadata": {},
   "source": [
    "### Part 2c: Create a data splitting strategy\n",
    "\n",
    "Next we create a data splitting strategy. Data splitting refers to the process of dividing data into three groups: training, validation, and testing. Each of these groups represent a part of the iterative process for machine learning model development. \n",
    "\n",
    "- Training data is the largest subset, usually around 60-80% of the total data, and is used to initially train the model. \n",
    "- Validation data is roughly 10-20% of the total data, and is used to validate the effectiveness of the training process. \n",
    "- Testing data is also roughly 10-20% of the total data, and is used to test the final refined model before using it on new, unseen data.\n",
    "\n",
    "Each group should be separate to ensure no single group will bias the model. Sam and the team used the following percentages in their original model:\n",
    "\n",
    "|    Group   | Percent of total data |\n",
    "|------------|-----------------------|\n",
    "| Training   | 75%                   |\n",
    "| Validation | 15%                   |\n",
    "| Testing    | 10%                   |\n",
    "\n",
    "> **Instructions**\n",
    "> \n",
    "> Execute the two cells below.\n",
    ">\n",
    "> In the Dataset Split Percentages widget, input the percentages Sam used in their original model (above).\n",
    ">\n",
    "> Select Submit after making your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93156db-9ba0-478d-8a86-cdecef7f1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Module2_tech import create_percentage_widget\n",
    "\n",
    "widget, get_values = create_percentage_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used to grab the values from the widget above (no need to change)\n",
    "decimals = get_values()\n",
    "training = decimals['training']\n",
    "validation = decimals['validation']\n",
    "testing = decimals['testing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab2a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Module2_tech import train_val_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e77b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(df, \n",
    "                                                                      y_col='ptype',\n",
    "                                                                      train_size=training,\n",
    "                                                                      val_size=validation,\n",
    "                                                                      test_size=testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72742d33-21fe-44ca-a5b2-784462ee71f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cce25e-004c-48ae-a88d-c1224592445b",
   "metadata": {},
   "source": [
    "between these divider bars - move to splitting widget (above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d192eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Module2_tech import create_column_filter_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget, get_selected_columns = create_column_filter_widget()\n",
    "display(widget)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b01fe714-5deb-4dde-beea-3b014c738a9e",
   "metadata": {},
   "source": [
    "this (below) gets ingested in the splitting widget right?\n",
    "\n",
    "Scaling Your Data\n",
    "\n",
    "After we split the data, we must scale it. Scaling helps us make sure all features, or variables, contribute equally to the model. We will be using [scikit-learn's `StandardScaler()` function](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler), which standardizes our features by substracting the mean from each observation and then dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd67cc-ec26-4641-9a26-7634880c138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e531b58-ec31-4821-afc1-78c4c3623fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d5e5f-beac-44a5-ace9-8a200c796672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing and validation data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1370a67-5043-4e3a-ac05-edaf7e803cfe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf517b-916f-483f-b89f-3da2465aa6a1",
   "metadata": {},
   "source": [
    "## Part 3: Model Development\n",
    "Next begins the iterative process of creating, evaluating, and refining the machine learning model. You will start by recreating Sam's original model and critically evaluating its performance. Then, you will refine the model based on your own choices, keeping track of your trials in your Machine Learning Model Handbook. \n",
    "<center><a href=\"https://elearning.unidata.ucar.edu/dataeLearning/Cybertraining/analysis/media/RevisedFull_NOPROCESS_.png\" target=\"blank\"><img src=\"https://elearning.unidata.ucar.edu/dataeLearning/Cybertraining/analysis/media/RevisedFull_NOPROCESS_.png\"></a><br><i>Click to enlarge</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc2dd6-8b11-42ee-94d3-62ba4efd8378",
   "metadata": {},
   "source": [
    "### Part 3a: Choose Algorithm\n",
    "\n",
    "Sam and the team chose the **Scikit-Learn LogisticRegression** algorithm to train their original model. Despite its name, this algorithm is used for classification tasks. This is because at its core, this algorithm predicts the likelihood of a record being rain or snow. It then returns the most likely classification. \n",
    "\n",
    "`graphical representation??`\n",
    "\n",
    "While you will first use this algorithm to recreate the original model, Sam and the team are interested in exploring other algorithms as well. You will have an opportunity later to test other options. \n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Module2_tech import algorithm_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca09e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_algo = algorithm_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91769fcb-0e65-4c6f-ad39-92c71a3fef9f",
   "metadata": {},
   "source": [
    "### Part 3b: Choose input features\n",
    "\n",
    "To recreate Sam's model, you'll use the same input features they used to create their original model. The features they chose are in the table below.\n",
    "\n",
    "| Sam's input features |\n",
    "|----------------------|\n",
    "| TEMP_C_0_m           |\n",
    "| T_DEWPOINT_C_0_m     |\n",
    "| UGRD_m/s_0_m         |\n",
    "| VGRD_m/s_0_m         |\n",
    "| PRES_Pa_0_m          |\n",
    "\n",
    "> **Instructions**\n",
    "> \n",
    "> Execute the two cells below.\n",
    ">\n",
    "> After executing `create_station_selector()`, select the input features used in Sam's model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b03f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a873f60-c146-404b-8aa3-3ae782a795a4",
   "metadata": {},
   "source": [
    "# from module 3\n",
    "\n",
    "from button_tech import create_station_selector\n",
    "\n",
    "station_selector = create_station_selector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f88bb38-fa7f-4abe-8e19-f00120c6d0f5",
   "metadata": {},
   "source": [
    "> **Instructions**\n",
    "> \n",
    "> Execute the cell below to commit your input feature selection. The output will also be used in describing subsequent evaluation metrics. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8d2f08f-6dda-45d0-b787-c73cf1c3bf31",
   "metadata": {},
   "source": [
    "# from module 3\n",
    "\n",
    "# To get selected stations at any time:\n",
    "def get_selected_stations(selector):\n",
    "    return [station for station, checkbox in selector.items() if checkbox.value]\n",
    "\n",
    "selected = get_selected_stations(station_selector)\n",
    "selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16715431-6fc1-4dad-aa6c-2b073b07eab7",
   "metadata": {},
   "source": [
    "This next block of code takes the full dataset and removes (filters) any stations that were not selected above. We do this for all groups (training, validation, and testing). \n",
    "\n",
    "<br>\n",
    "\n",
    "> **Instructions**\n",
    "> \n",
    "> Execute the two cells below. In the printout display, you will see the number of features (columns) in the original dataset, and the number of features in the filtered dataset. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "042d70d2-1f48-43e0-abdf-6e44934c77a4",
   "metadata": {},
   "source": [
    "# from module 3\n",
    "\n",
    "from button_tech import filter_dataframe"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96181b39-4d8c-4a6f-a100-ffdb6098ede6",
   "metadata": {},
   "source": [
    "# from module 3\n",
    "\n",
    "X_train_filtered     = filter_dataframe(X_train,     selected)\n",
    "X_val_filtered       = filter_dataframe(X_val,       selected)\n",
    "X_test_filtered      = filter_dataframe(X_test,      selected)\n",
    "X_true_test_filtered = filter_dataframe(X_true_test, selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262339ca-3763-436d-82be-499961b829cc",
   "metadata": {},
   "source": [
    "### Part 3c: Train the Algorithm\n",
    "\n",
    "The training process is what transforms the machine learning algorithm into a supervised machine learning model. You will now train the algorithm to recreate Sam's model.\n",
    "\n",
    "> **Instructions**\n",
    "> \n",
    "> Execute the two cells below.\n",
    ">\n",
    "> After executing `train_button()`, select the Train Algorithm button to initiate the training process. A progress printout will display below the button while the process runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca993c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Module2_tech import train_button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44621f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choice = selected_algo()\n",
    "trained_model = train_button(model_choice, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5b7ab-6f93-4721-8d0d-9ef075276a0d",
   "metadata": {},
   "source": [
    "### Part 3d: Validate the Model\n",
    "\n",
    "The validation step uses the separate validation dataset to evaluate how well the training process performed. The newly trained model takes the environmental variable input features from the validation dataset and outputs a rain or snow classification prediction. We can then take the predictions the model made and compare them to the known *true* classifications. By using a separate validation dataset to evaluate performance, we get a better sense of how well the model can generalize to new inputs. \n",
    "\n",
    "Sam's team calculated the model's **accuracy** to determine how well the model performed, which you will also do in this step. Accuracy is defined as \n",
    "$$ \\frac{\\text{correct predictions}}{\\text{total predictions}} $$\n",
    "\n",
    "\n",
    "> **Instructions**\n",
    "> \n",
    "> Execute the two cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd8302-3ffa-4f16-a89d-086b4abff548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = trained_model().predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Get accuracy metric\n",
    "accuracy = (accuracy_score(y_test, y_pred))*100\n",
    "print(f\"Original Model Accuracy (validation dataset): {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca97a1-566f-493a-a003-1855215ff293",
   "metadata": {},
   "source": [
    "#### Other evaluation metrics\n",
    "While accuracy gives a broad view of how well the model performs, we don't know the specifics. Does the model struggle to classify just rain, just snow, or both? You will calculate additional statistics for Sam and the team to provide additional insight into the model's performance. \n",
    "\n",
    "##### Confusion matrix\n",
    "A confusion matrix is a visual representation of all the predictions the model made. In this two-class model, there can be four kinds of outputs:\n",
    "\n",
    "- Predicted rain, and the true observation (target feature) was rain \n",
    "- Predicted rain, but the true observation was snow\n",
    "- Predicted snow, but the true observation was rain \n",
    "- Predicted snow, and the true observation was snow \n",
    "\n",
    "The confusion matrix shows the number of each of these types outputs in a grid, so we can visualize the predictions that the model generated. \n",
    "\n",
    "\n",
    "> **Instructions**\n",
    "> \n",
    "> Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad04c1a-49ca-4e57-ace3-6b1c8aebdcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = trained_model().classes_\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create heatmap with labels\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels,\n",
    "            cbar=False)\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('True Observation')\n",
    "plt.title('Original Model Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c506497-634f-4e21-9933-d197b5a576ed",
   "metadata": {},
   "source": [
    "The values in the confusion matrix can then be used to quantify additional model evaluation metrics. These metrics help answer specific questions about how well the model performs. \n",
    "\n",
    "##### Precision\n",
    "Precision is a measure of correct predictions for each class. It answers the question *\"When the model predicts rain (or snow), how often is it correct?\"* We calculate the precision of the rain class by\n",
    "\n",
    "$$ \\frac{\\text{correct rain predictions}}{\\text{correct rain predictions + incorrect rain predictions}} $$\n",
    "\n",
    "For example, a rain precision value of 0.65 would tell us that of the times that the model predicted rain, it was correct only 65% of the time. A high-performing model would have a precision value close to 1.\n",
    "\n",
    "##### Recall\n",
    "Recall is a measure of how well the model predicts the correct classification for each class. It answers the question *\"Out of all actual rain (or snow) cases, how many did the model correctly predict?\"* We calculate it as\n",
    "\n",
    "$$ \\frac{\\text{correct rain predictions}}{\\text{total actual rain cases}} $$\n",
    "\n",
    "For example, a rain recall value of .81 would tell us that of all of the true rain cases in the dataset, the model correctly predicted rain in 81% of them. A high-performing model would have a recall value close to 1. \n",
    "\n",
    "##### Comparing Precision and Recall\n",
    "These metrics may seem very similar, but they differ in the types of questions they answer.\n",
    "|                          | High Precision, Low Recall           | High Recall, Low Precision          |\n",
    "|--------------------------|---------------------------------|---------------------------------|\n",
    "| **What happens?**        | The model is **very careful** when predicting rain, but it **misses** a lot of actual rain cases. | The model **catches almost all rain cases**, but it **also makes mistakes**, calling snow \"rain\" a lot. |\n",
    "| **Example**             | Predicts \"rain\" only when very sure, leading to fewer false alarms but missing real rain. | Predicts \"rain\" too often, ensuring real rain isn’t missed but causing more false alarms. |\n",
    "\n",
    "<br><br>\n",
    "> **Instructions**\n",
    "> \n",
    "> Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e170b0-9502-43b3-96d3-dde0ea1ac9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average=None, labels=['rain', 'snow'])\n",
    "\n",
    "print(\"Original Model Validation Metrics\")\n",
    "print(f\"Rain Precision: {precision[0]:.3f}\")\n",
    "print(f\"Snow Precision: {precision[1]:.3f}\")\n",
    "print(f\"Rain Recall: {recall[0]:.3f}\")\n",
    "print(f\"Snow Recall: {recall[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c12a24-b114-4cb9-9e89-f2a40de4fec0",
   "metadata": {},
   "source": [
    "### Part 3e: Evaluate and Refine the Model\n",
    "\n",
    "Examine the results of the original model validation. What do each mean? Review the descriptions of the evaluation metrics, then complete the next exercise. \n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<p class=\"admonition-title\" style=\"font-weight:bold\">Exercise 3e</p>\n",
    "    <p>Open your <b>Machine Learning Model Handbook</b> to Exercise 3e. </p>\n",
    "    <p>Paste the validation evaluation metrics in the designated box. </p>\n",
    "    <p>Then describe the results of the original model validation. Include the following:</p>\n",
    "    <ul>\n",
    "        <li>How well does the model predict rain? Support your description with the evaluation metrics. </li>\n",
    "        <li>How well does the model predict snow? Support your description with the evaluation metrics. </li>\n",
    "        <li>How do you interpret these statistics in the context of the physical world?</li>\n",
    "        <li>What changes will you make to try to improve these statistics in the next iteration?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e04a32-e8f9-4a6c-b496-14f7188c4f26",
   "metadata": {},
   "source": [
    "### Part 3f: Iterative Refinement Trials\n",
    "\n",
    "Now that you have recreated Sam's original model, it's now your turn to apply the information you have to refine it. Next,  you'll create new trials to improve the evaluation metrics from the validation phase. You may complete as many trials as you like until you are satisfied with the evaluation metrics, or they no longer improve with new trials. \n",
    "\n",
    "> **Instructions**\n",
    "> \n",
    "> Execute the code cells below, selecting your desired model configurations after executing each cell.\n",
    "> \n",
    "> After each new trial, you will copy the validation metrics in your handbook document. See **Exercise 3f**.\n",
    ">\n",
    "> You may complete as many trials in this section (3f) as you like until you are satisfied with the evaluation metrics, or they no longer improve with new trials. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047ec698-74f5-466b-8d53-75c6559ea289",
   "metadata": {},
   "source": [
    "#### New trial: Choose algorithm \n",
    "\n",
    "In the new trials, you have the option of choosing a new algorithm to train. Review the information about each of the algorithms, then make your selection. Remember, you can run as many trials as you'd like. \n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">About the Algorithms</p>\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Big Algo</p>\n",
    "    <ul>\n",
    "        <li>A pro</li>\n",
    "        <li>A con </li>\n",
    "    </ul>\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Algo(TM)</p>\n",
    "    <ul>\n",
    "        <li>A pro</li>\n",
    "        <li>A con</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e29d0baa-ce71-48ba-a065-43cafc6c6f30",
   "metadata": {},
   "source": [
    "selected_algo = algorithm_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20354af5-dc7a-4f23-9313-abb9eed8954b",
   "metadata": {},
   "source": [
    "#### New trial: Choose input features\n",
    "\n",
    "Sam used environmental variables at the surface level to train the algorithm. The dataset also includes environmental variables at other levels that may provide additional predictive skill. Choose as many input features as you like, but recall that *more data doesn't always produce a better model*. Be strategic in your selections. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a48684fe-a4ae-46f9-bac0-23ff7860e302",
   "metadata": {},
   "source": [
    "station_selector = create_station_selector()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7e2d5f4-4cca-4760-8984-c3030b84ad91",
   "metadata": {},
   "source": [
    "# Execute this cell after selecting stations\n",
    "selected = get_selected_stations(station_selector)\n",
    "X_train_filtered     = filter_dataframe(X_train,     selected)\n",
    "X_val_filtered       = filter_dataframe(X_val,       selected)\n",
    "X_test_filtered      = filter_dataframe(X_test,      selected)\n",
    "X_true_test_filtered = filter_dataframe(X_true_test, selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc8fb7-f202-4843-99bd-f9a9aee30e02",
   "metadata": {},
   "source": [
    "#### New trial: Train algorithm\n",
    "\n",
    "Now, train the algorithm with your selected input features"
   ]
  },
  {
   "cell_type": "raw",
   "id": "896145d2-2479-423d-937e-d2229f284292",
   "metadata": {},
   "source": [
    "model_choice = selected_algo()\n",
    "trained_model = train_button(model_choice, X_train_filtered, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f29327-640e-4a44-ad0a-fe5e635aa55d",
   "metadata": {},
   "source": [
    "#### New trial: Validate model\n",
    "\n",
    "Generate the corresponding confusion matrix and validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f9cba-6145-4209-af63-52a272f1054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might be nice to have an all-in-one function like \n",
    "# model_eval_MITC(trained_model(), X_test_filtered, y_test)\n",
    "class_labels = model.classes_\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create heatmap with labels\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels,\n",
    "            cbar=False)\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('True Observation')\n",
    "plt.title('New Trial Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average=None, labels=['rain', 'snow'])\n",
    "accuracy = (accuracy_score(y_test, y_pred))*100\n",
    "\n",
    "print(\"New Trial Validation Metrics\")\n",
    "print(\"Used this algo + these input features\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Rain Precision: {precision[0]:.3f}\")\n",
    "print(f\"Snow Precision: {precision[1]:.3f}\")\n",
    "print(f\"Rain Recall: {recall[0]:.3f}\")\n",
    "print(f\"Snow Recall: {recall[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c25560-246a-4e55-9c95-4cee15f02b24",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<p class=\"admonition-title\" style=\"font-weight:bold\">Exercise 3f</p>\n",
    "    <ul>\n",
    "        <li><b>In your Machine Learning Model Handbook Exercise 3f,</b> paste the full output of each of your validation trials, one per box. </li>\n",
    "        <li>You may complete as many trials as you like until you are satisfied with the evaluation metrics, or they no longer improve with new trials. When complete, move on to the next part below. </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49c27b-518a-4e6a-a1dd-0cdb00f862cd",
   "metadata": {},
   "source": [
    "### Part 3g: Test Model\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<p class=\"admonition-title\" style=\"font-weight:bold\">Important</p>\n",
    "    For testing, your model needs to be in a state with your desired algorithm and input features. If you haven't already, go back and run through the cells in Part 3f with your final choices one last time. This ensures that your final testing process will be executed with your desired choices. \n",
    "</div>\n",
    "<br>\n",
    "At this point, you have a trained model with validation metrics you are satisfied with. Next, it's time to test the model on brand new data: the testing dataset. The testing process mimics how the model would be used in a real-world process in a final, unbiased way. \n",
    "\n",
    "Testing looks very similar to validation. You will again review the confusion matrix, accuracy, precision, and recall, but this time the predictions are made with the testing datset. \n",
    "\n",
    "> **Instructions**\n",
    "> \n",
    "> Execute the cell below.\n",
    ">\n",
    "> After executing `model_eval_MITC()`, your model's testing metrics will appear below as a printout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6744f3-f5db-41c2-87b0-15dfbb6836a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Validation set results\n",
    "y_val_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# metrics\n",
    "class_labels = model.classes_\n",
    "\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Create heatmap with labels\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels,\n",
    "            cbar=False)\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('True Observation')\n",
    "plt.title('New Trial Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "precision, recall, _, _ = precision_recall_fscore_support(y_val, y_val_pred, average=None, labels=['rain', 'snow'])\n",
    "accuracy = (accuracy_score(y_val, y_val_pred))*100\n",
    "\n",
    "print(\"New Trial Validation Metrics\")\n",
    "print(\"Used this algo + these input features\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Rain Precision: {precision[0]:.3f}\")\n",
    "print(f\"Snow Precision: {precision[1]:.3f}\")\n",
    "print(f\"Rain Recall: {recall[0]:.3f}\")\n",
    "print(f\"Snow Recall: {recall[1]:.3f}\")\n",
    "\n",
    "# Get various metrics for validation set\n",
    "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
    "precision_val, recall_val, f1_val, _ = precision_recall_fscore_support(y_val, y_val_pred, average='binary', pos_label='rain')\n",
    "\n",
    "print(f\"Accuracy on Validation Set: {accuracy_val:.3f}\")\n",
    "print(f\"Precision on Validation Set: {precision_val:.3f}\")\n",
    "print(f\"Recall on Validation Set: {recall_val:.3f}\")\n",
    "print(f\"F1 Score on Validation Set: {f1_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3a36e-7236-4894-b9e7-24cc8ee72be6",
   "metadata": {},
   "source": [
    "### Part 3h: Evaluate and Justify\n",
    "\n",
    "#### Your final decision\n",
    "\n",
    "Given all your evaluation, it's time to make a final recommendation to Sam and the team on whether you believe this model provides sufficient skill for the needs of the situation. Go back and review the problem statement. Does this model deliver the results needed?\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<p class=\"admonition-title\" style=\"font-weight:bold\">Exercise 3h</p>\n",
    "    <p>Open your <b>Machine Learning Model Handbook</b> to Exercise 3h.</p>\n",
    "    <p>Paste your testing evaluation metrics in the designated box. </p>\n",
    "    <p>Then  make a final decision on whether this model delivers on the results needed with supporting justification. Include the following:\n",
    "    <ul>\n",
    "        <li>Which precipitation class(es) had the best evaluation metrics? List some physical scientific reasons why this may be the case.</li>\n",
    "        <li>Is this model ready for use in the real world? Why or Why not?</li>\n",
    "        <li>What other possible changes could further improve this model?</li>\n",
    "    </ul>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3eade0-4ae9-4b2f-84d0-0487078173a5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ae3f9da-c262-487a-b90d-fe7e6424b431",
   "metadata": {},
   "source": [
    "delete all below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6803e64e-b857-44d1-a1fa-bc4a2797355b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39dbc4-a6bc-4cd1-ac9a-d9385fe6ee65",
   "metadata": {},
   "source": [
    "## Different dataset, different results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c963b-c722-4b5c-9a72-8c76d71c8636",
   "metadata": {},
   "source": [
    "Let's look at another dataset. This dataset just has snow and freezing rain as the p-types, so overall it will be colder. Let's see if we get similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6e11b-2fef-451a-afec-06815023fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frza = pd.read_parquet(r'../ptype_data/ptype_sampled_frza.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057f39e-38a6-413d-bdde-ebf293706207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frza.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16058d5-24f6-44f5-8a57-b0e71861c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_frza, hue='ptype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe70d6-cb07-4dee-9ca1-9137a724975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_frza.select_dtypes(include=['number']).corr(), vmin=-1, vmax=1, cmap='coolwarm', annot=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd40d1-786d-4483-8e4a-cf48e0aa5e8a",
   "metadata": {},
   "source": [
    "What are the differences in EDA compared to rain vs snow? Do you expect this to do better or worse compared to rain vs snow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c4863-b17a-453d-b749-9f83399d0023",
   "metadata": {},
   "source": [
    "### Split up the data & Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0d666-d457-4fb1-9bf1-9288d3e204c3",
   "metadata": {},
   "source": [
    "The following code is a bit compressed, but is the same line for line as above, just with a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b8853-db4b-4462-ac15-e1613a5010d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df_frza[['TEMP_C_0_m','UGRD_m/s_0_m','T_DEWPOINT_C_0_m', 'VGRD_m/s_0_m', 'PRES_Pa_0_m' ]]\n",
    "y2 = df_frza['ptype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826e4353-c041-4a58-a9c7-40b39f5e0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and temporary set (70% training, 30% temporary)\n",
    "X_train2, X_temp2, y_train2, y_temp2 = train_test_split(X2, y2, test_size=0.3, random_state=42)\n",
    "\n",
    "# Splitting the temporary set into testing and validation sets (20% testing, 10% validation of the original dataset)\n",
    "X_test2, X_val2, y_test2, y_val2 = train_test_split(X_temp2, y_temp2, test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ab5cb-24aa-4972-9612-3aaffa03b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb95efd5-c61f-43ae-9ac5-41183e1af6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled2 = scaler.fit_transform(X_train2)\n",
    "\n",
    "# Transform the testing and validation data using the same scaler\n",
    "X_test_scaled2 = scaler.transform(X_test2)\n",
    "X_val_scaled2 = scaler.transform(X_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b3ad0",
   "metadata": {},
   "source": [
    "Notice the new model! We will be using a [Decision Tree](https://scikit-learn.org/stable/modules/tree.html). If you want to learn more, here is a StatQuest [video](https://www.youtube.com/watch?v=_L39rN6gz7Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e19ddb-5b08-462b-bb3f-aecd05da0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93feb8e8-9cca-4fb7-9834-bd1acb3a1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the training data\n",
    "model.fit(X_train_scaled2, y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db325773-5ff7-456c-8b03-c406c027cc00",
   "metadata": {},
   "source": [
    "### Test Set ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e652eb1-713c-43df-856c-942401f85830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred2 = model.predict(X_test_scaled2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e571d2-c250-44d1-88bf-3e52c097ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification metrics for test set\n",
    "accuracy = accuracy_score(y_test2, y_pred2)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test2, y_pred2, average='binary', pos_label='snow')\n",
    "\n",
    "print(f\"Accuracy on Test Set: {accuracy:.3f}\")\n",
    "print(f\"Precision on Test Set: {precision:.3f}\")\n",
    "print(f\"Recall on Test Set: {recall:.3f}\")\n",
    "print(f\"F1 Score on Test Set: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e580c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique class labels\n",
    "class_labels = model.classes_  # If using sklearn model\n",
    "# OR\n",
    "class_labels = np.unique(y_test2)  # Alternative way\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test2, y_pred2)\n",
    "\n",
    "# Create heatmap with labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01ed0e",
   "metadata": {},
   "source": [
    "What do you notice in this confusion matrix compared to the one with rain vs snow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7870004-6af3-47b9-bf08-5540bf521f10",
   "metadata": {},
   "source": [
    "### Validation Set ML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8a36c-849f-4c7d-9559-d55c0a39781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Validation set results\n",
    "y_val_pred2 = model.predict(X_val_scaled2)\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy_val = accuracy_score(y_val2, y_val_pred2)\n",
    "precision_val, recall_val, f1_val, _ = precision_recall_fscore_support(y_val2, y_val_pred2, average='binary', pos_label='fzra')\n",
    "\n",
    "print(f\"Accuracy on Validation Set: {accuracy_val:.3f}\")\n",
    "print(f\"Precision on Validation Set: {precision_val:.3f}\")\n",
    "print(f\"Recall on Validation Set: {recall_val:.3f}\")\n",
    "print(f\"F1 Score on Validation Set: {f1_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653dc95-65bc-4c67-8418-138e33b5cfdf",
   "metadata": {},
   "source": [
    "### More Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ba09c-c2db-4e76-8c65-fb4db72ed395",
   "metadata": {},
   "source": [
    "1. What do you see comparing the metrics; freezing rain vs snow and snow vs rain? Is this what you expected?\n",
    "   \n",
    "1. Is the Decision Tree model consistent between testing and validations sets for both experiments? Could we potentially use a more complex model?\n",
    "   \n",
    "1. How many lines of code does it take to do a quick ML analysis with a testing, training, and validation dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5244e2-a2de-427a-8809-4b9c1465de01",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a3f4de-0a67-46ca-9a99-a145fed2395f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook we learned:\n",
    "1. What Exploratory Data Analysis is and some useful functions that can help you in the process of understading your data.\n",
    "1. How and why we split and scale data\n",
    "1. How to train your model and evaluate its accuracy afterwards\n",
    "\n",
    "### What's next?\n",
    "Let Jupyter book tie this to the next (sequential) piece of content that people could move on to down below and in the sidebar. However, if this page uniquely enables your reader to tackle other nonsequential concepts throughout this book, or even external content, link to it here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de471f53-96bc-48b1-ba03-e116ae6280af",
   "metadata": {},
   "source": [
    "## Resources and references\n",
    "1. [Scikit-learn](https://scikit-learn.org/stable/)\n",
    "1. [Correlation Matrix, Demystified](https://towardsdatascience.com/correlation-matrix-demystified-3ae3405c86c1)\n",
    "1. [What is the Difference Between Test and Validation Datasets?](https://machinelearningmastery.com/difference-test-validation-datasets/)\n",
    "1. [Machine Learning Foundations in the Earth Systems Sciences](https://elearning.unidata.ucar.edu/dataeLearning/Cybertraining/foundations/)\n",
    "1. [Scikit-learn's StandardScaler Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)\n",
    "1. [What and why behind fit_transform() and transform() in scikit-learn!](https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe)\n",
    "1. [is\n",
    "R2: Downsides and Potential Pitfalls for ESS ML Predic](https://www.unidata.ucar.edu/blogs/news/entry/r-sup-2-sup-downsides)\n",
    "1. [Scikit-learn's Decision Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "1. [StatQuest video: Decision and Classification Trees, Clearly Explained!!!](https://www.youtube.com/watch?v=_L39rN6gz7Y)tion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
