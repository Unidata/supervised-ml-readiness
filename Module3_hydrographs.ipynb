{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrograph ML\n",
    "Last edited by NC\n",
    "TAKE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we predict the hydograph downsteam, using upstream weirs and datasets?\n",
    "\n",
    "This will be for Golden, CO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import metpy for meteorological data analysis and visualization\n",
    "import metpy\n",
    "\n",
    "# Import dataretrieval for accessing environmental data from various sources\n",
    "import dataretrieval\n",
    "import dataretrieval.nwis as nwis\n",
    "\n",
    "# Import cartopy for cartographic projections and geographic data visualization\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# Import USCOUNTIES from metpy.plots for plotting US county boundaries\n",
    "from metpy.plots import USCOUNTIES\n",
    "\n",
    "# Import gridliner from cartopy.mpl for formatting gridlines on maps\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('What version of Dataretrieval?', dataretrieval.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the USGS site code for which we want data.\n",
    "site = ['06719505', # Clear Creek at Golden\n",
    "        '06718550', # North Clear Creek @ BlackHawk\n",
    "        '06716500',\n",
    "        '06714800', # Leavenworth Creek at Mouth Near Georgetown, CO\n",
    "        '06716100'] # West Fork Clear Creek Abv Mouth NR Empire, CO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get instantaneous values (iv)\n",
    "df = nwis.get_record(sites=site, service='iv', start='2011-1-01', end='2023-12-30')\n",
    "\n",
    "# Want some metadata\n",
    "df_site = nwis.get_record(sites=site, service='site')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some of these columns?\n",
    "\n",
    "00060 = Daily Mean Discharge\n",
    "\n",
    "00065 = Gage height, feet\n",
    "\n",
    "_cd = condition code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a site map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are we in the world :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure CRS matches the data\n",
    "proj = ccrs.LambertConformal(central_longitude=-105.5, central_latitude=39.7)\n",
    "\n",
    "# Create the figure and axis\n",
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax1 = fig.add_subplot(1, 1, 1, projection=proj)\n",
    "\n",
    "# Set map extent to focus on Florida\n",
    "ax1.set_extent([-106, -104.5, 39, 40.5], ccrs.Geodetic())  # Adjust the extent to focus on Florida\n",
    "\n",
    "# Add map features\n",
    "ax1.add_feature(USCOUNTIES.with_scale('500k'))\n",
    "\n",
    "# Scatter plot\n",
    "scatter = ax1.scatter(df_site.dec_long_va, df_site.dec_lat_va, color='red', marker='o', s=100, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Add legend\n",
    "ax1.legend([scatter], ['Gauge Locations'], loc='lower left')  # Adjust the legend label as needed\n",
    "\n",
    "# Add gridlines with tick marks\n",
    "gl = ax1.gridlines(draw_labels=True)\n",
    "gl.xlabels_top = False\n",
    "gl.ylabels_right = False\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'size': 12}\n",
    "gl.ylabel_style = {'size': 12}\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's dig into the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I personally hate multi-index, but to each their own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to break out year, day of year, and hour of the day into it's own column. This is just to make it easier to parse later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['datetime'].dt.year\n",
    "df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "df['hour_of_day'] = df['datetime'].dt.hour\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will not be using actual datetime anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('datetime', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_shape = df.shape[0]\n",
    "df = df.drop_duplicates(keep='first')  # Explicitly keep the first occurrence\n",
    "new_shape = df.shape[0]\n",
    "print(f\"Removed {original_shape - new_shape} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can make a quick crossplot comparing daily mean discharge to gauge height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['00060'],\n",
    "            df['00065'], \n",
    "            alpha=0.05)\n",
    "\n",
    "plt.xlabel('Discharge (cfs)')  # Label for x-axis\n",
    "plt.ylabel('Gauge Height (feet)')  # Label for y-axis\n",
    "plt.title('USGS Streamflow Data')  # Title for the plot\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique site numbers from the DataFrame\n",
    "unique_site_numbers = df['site_no'].unique()\n",
    "\n",
    "# Get the colormap and generate the required number of colors\n",
    "cmap = plt.colormaps.get_cmap('tab10')  # Get the colormap\n",
    "colors = [cmap(i / len(unique_site_numbers)) for i in range(len(unique_site_numbers))]  # Generate the colors\n",
    "\n",
    "# Create a dictionary to map each site number to a color\n",
    "site_color_map = {site: colors[i] for i, site in enumerate(unique_site_numbers)}\n",
    "\n",
    "# Scatter plot with colors based on site numbers\n",
    "plt.scatter(df['00060'],\n",
    "            df['00065'],\n",
    "            c=df['site_no'].map(site_color_map),  # Map each site to its corresponding color\n",
    "            alpha=0.5,\n",
    "            s=7)\n",
    "\n",
    "plt.xlabel('Discharge (cfs)')  # Label for x-axis\n",
    "plt.ylabel('Gauge Height (feet)')  # Label for y-axis\n",
    "plt.title('USGS Streamflow Data')  # Title for the plot\n",
    "\n",
    "plt.xscale('log')  # Applying log scale to the x-axis\n",
    "\n",
    "# Add a legend\n",
    "legend_handles = [plt.Line2D([0], [0], marker='o', color='w', label=site, markerfacecolor=color, markersize=5) for site, color in site_color_map.items()]\n",
    "plt.legend(handles=legend_handles)\n",
    "\n",
    "plt.grid(True)  # Adding a grid to the plot\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique site numbers\n",
    "unique_site_numbers = df['site_no'].unique()\n",
    "\n",
    "# Determine the number of rows and columns for subplots\n",
    "num_rows = len(unique_site_numbers)\n",
    "num_cols = 1  # One column for each site\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 5*num_rows))\n",
    "\n",
    "# Plot data for each site\n",
    "for i, site_number in enumerate(unique_site_numbers):\n",
    "    # Filter data for the specific site\n",
    "    site_data = df[df['site_no'] == site_number]\n",
    "    \n",
    "    # Plot data on the corresponding subplot\n",
    "    sns.lineplot(x='day_of_year', y='00060', data=site_data, hue='year', ax=axes[i], alpha=0.5)\n",
    "    \n",
    "    # Calculate median discharge for each day of the year\n",
    "    median_discharge = site_data.groupby('day_of_year')['00060'].median()\n",
    "    \n",
    "    # Plot median line on the same subplot\n",
    "    axes[i].plot(median_discharge.index, median_discharge.values, color='black', linestyle='--', label='Median', linewidth=2)\n",
    "    \n",
    "    # Smooth the median discharge using a 7-point convolution\n",
    "    smoothed_discharge = np.convolve(median_discharge, np.ones(14)/14, mode='same')\n",
    "    \n",
    "    # Plot smoothed line on the same subplot\n",
    "    axes[i].plot(median_discharge.index, smoothed_discharge, color='red', linestyle=':', label='Smoothed', linewidth=2)\n",
    "    \n",
    "    # Customize subplot\n",
    "    axes[i].set_xlabel('Day of Year')\n",
    "    axes[i].set_ylabel('Discharge (cfs)')\n",
    "    axes[i].set_title(f'Site {site_number}')\n",
    "    axes[i].legend(title='Year', loc='upper left')\n",
    "    axes[i].tick_params(axis='x', rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = df.isna().sum()\n",
    "nan_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = df['year'].value_counts()\n",
    "unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar \n",
    "\n",
    "# Assuming df.day_of_year is your data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df.day_of_year, bins=52, density=True, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Improve x-axis labels\n",
    "months = calendar.month_abbr[1:]\n",
    "plt.xticks(np.arange(15, 365, 30.5), months, rotation=45)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Normalized Frequency')\n",
    "plt.title('Distribution of Events Throughout the Year')\n",
    "\n",
    "# Add grid\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how many nan's per station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in absolute terms\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a percentage\n",
    "(df.isna().sum() / len(df)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nans_per_station(df):\n",
    "    # Get all columns except 'site_no'\n",
    "    columns_to_check = [col for col in df.columns if col != 'site_no']\n",
    "    \n",
    "    # Initialize the result DataFrame with 'site_no'\n",
    "    result = df['site_no'].drop_duplicates().to_frame()\n",
    "    \n",
    "    for column in columns_to_check:\n",
    "        # Group by 'site_no' and sum NaN values for the current column\n",
    "        column_nans = df.groupby('site_no')[column].apply(lambda x: x.isna().sum()).reset_index()\n",
    "        column_nans.columns = ['site_no', f'{column}_nan_count']\n",
    "        \n",
    "        # Merge with the result DataFrame\n",
    "        result = result.merge(column_nans, on='site_no', how='left')\n",
    "    \n",
    "    # Add total NaN count per station\n",
    "    nan_columns = [col for col in result.columns if col.endswith('_nan_count')]\n",
    "    result['total_nan_count'] = result[nan_columns].sum(axis=1)\n",
    "    \n",
    "    # Sort by total NaN count in descending order\n",
    "    result = result.sort_values('total_nan_count', ascending=False)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans_per_station(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will want to impute the data with the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_median(df, column):\n",
    "    # Store the original NaN count\n",
    "    original_nan_count = df[column].isna().sum()\n",
    "    \n",
    "    # Try filling with median for same site, day, and hour\n",
    "    df[column] = df.groupby(['site_no', 'day_of_year', 'hour_of_day'])[column].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    # If still NaN, try filling with median for same site and day\n",
    "    df[column] = df.groupby(['site_no', 'day_of_year'])[column].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    # If still NaN, try filling with median for same site\n",
    "    df[column] = df.groupby(['site_no'])[column].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    # If still NaN, fill with overall median\n",
    "    remaining_nan_count = df[column].isna().sum()\n",
    "    if remaining_nan_count > 0:\n",
    "        print(f\"Using overall median to fill {remaining_nan_count} NaN values in column '{column}'\")\n",
    "        df[column] = df[column].fillna(df[column].median())\n",
    "    \n",
    "    final_nan_count = df[column].isna().sum()\n",
    "    print(f\"Column '{column}': {original_nan_count} NaNs initially, {final_nan_count} NaNs remaining\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = fill_with_median(df, '00060')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique site numbers from the DataFrame\n",
    "unique_site_numbers = df['site_no'].unique()\n",
    "\n",
    "# Get the colormap and generate the required number of colors\n",
    "cmap = plt.colormaps.get_cmap('tab10')  # Get the colormap\n",
    "colors = [cmap(i / len(unique_site_numbers)) for i in range(len(unique_site_numbers))]  # Generate the colors\n",
    "\n",
    "# Create a dictionary to map each site number to a color\n",
    "site_color_map = {site: colors[i] for i, site in enumerate(unique_site_numbers)}\n",
    "\n",
    "# Scatter plot with colors based on site numbers\n",
    "plt.scatter(df2['00060'],\n",
    "            df2['00065'],\n",
    "            c=df2['site_no'].map(site_color_map),  # Map each site to its corresponding color\n",
    "            alpha=0.7,\n",
    "            s=7)\n",
    "\n",
    "plt.xlabel('Discharge (cfs)')  # Label for x-axis\n",
    "plt.ylabel('Gauge Height (feet)')  # Label for y-axis\n",
    "plt.title('USGS Streamflow Data')  # Title for the plot\n",
    "\n",
    "plt.xscale('log')  # Applying log scale to the x-axis\n",
    "\n",
    "# Add a legend\n",
    "legend_handles = [plt.Line2D([0], [0], marker='o', color='w', label=site, markerfacecolor=color, markersize=5) for site, color in site_color_map.items()]\n",
    "plt.legend(handles=legend_handles)\n",
    "\n",
    "plt.grid(True)  # Adding a grid to the plot\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.drop(['00065','00065_cd', '00060_cd'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirming how many nans now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_shape = df3.shape[0]\n",
    "df3 = df3.drop_duplicates(keep='first')  # Explicitly keep the first occurrence\n",
    "new_shape = df3.shape[0]\n",
    "print(f\"Removed {original_shape - new_shape} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df3.isna().sum() / len(df3)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of unique stations\n",
    "unique_stations_list = df3['site_no'].unique().tolist()\n",
    "\n",
    "# Initialize an empty DataFrame to store the merged results\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each station in the list of unique stations\n",
    "for station in unique_stations_list:\n",
    "    # Filter the data for the current station\n",
    "    station_df = df3[df3['site_no'] == station]\n",
    "    \n",
    "    # Check if the station data is not empty\n",
    "    if not station_df.empty:\n",
    "        # Rename the '00060' column to include the station's site_no\n",
    "        new_column_name = f'00060_{station}'\n",
    "        station_df = station_df.rename(columns={'00060': new_column_name})\n",
    "        \n",
    "        # Select only the columns we want to merge\n",
    "        columns_to_merge = ['year', 'day_of_year', 'hour_of_day', new_column_name]\n",
    "        station_df = station_df[columns_to_merge]\n",
    "        \n",
    "        # If merged_df is empty, this is the first valid data we've processed\n",
    "        if merged_df.empty:\n",
    "            merged_df = station_df\n",
    "        else:\n",
    "            # If merged_df already contains data, merge the new data\n",
    "            merged_df = pd.merge(merged_df, station_df, \n",
    "                                 on=['year', 'day_of_year', 'hour_of_day'], \n",
    "                                 how='outer')\n",
    "        \n",
    "        # Print a success message for this station\n",
    "        print(f\"Station {station} processed successfully!\")\n",
    "    else:\n",
    "        # If station_df is empty, print a message and skip this station\n",
    "        print(f\"Skipping empty result for station {station}\")\n",
    "\n",
    "# After the loop, sort the merged_df by year, day_of_year, and hour_of_day\n",
    "merged_df = merged_df.sort_values(['year', 'day_of_year', 'hour_of_day'])\n",
    "\n",
    "# Reset the index of the final merged DataFrame\n",
    "merged_df = merged_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_median_nosite(df, column):\n",
    "    # Extract site number from the column name\n",
    "    site_no = column.split('_')[1]\n",
    "    \n",
    "    # Store the original NaN count\n",
    "    original_nan_count = df[column].isna().sum()\n",
    "    \n",
    "    # Try filling with median for same day and hour\n",
    "    df[column] = df.groupby(['day_of_year', 'hour_of_day'])[column].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    # If still NaN, try filling with median for same day\n",
    "    df[column] = df.groupby(['day_of_year'])[column].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    # If still NaN, fill with overall median\n",
    "    remaining_nan_count = df[column].isna().sum()\n",
    "    if remaining_nan_count > 0:\n",
    "        print(f\"Using overall median to fill {remaining_nan_count} NaN values in column '{column}' (site {site_no})\")\n",
    "        df[column] = df[column].fillna(df[column].median())\n",
    "    \n",
    "    final_nan_count = df[column].isna().sum()\n",
    "    print(f\"Column '{column}' (site {site_no}): {original_nan_count} NaNs initially, {final_nan_count} NaNs remaining\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    '00060_06714800',\n",
    "    '00060_06716100', \n",
    "    '00060_06716500', \n",
    "    '00060_06718550', \n",
    "    '00060_06719505'\n",
    "]\n",
    "\n",
    "for column in columns:\n",
    "    merged_df = fill_with_median_nosite(merged_df, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the missing data per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(merged_df.isna().mean() * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some duplicate rows, we just want 1 answer per row (day of year, year, hour of day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to group by and the columns to average\n",
    "columns_to_group = ['year', 'day_of_year', 'hour_of_day']\n",
    "columns_to_average = ['00060_06714800', '00060_06716100', '00060_06716500', '00060_06718550', '00060_06719505']\n",
    "\n",
    "# Group by the specified columns and average the others\n",
    "result_df = merged_df.groupby(columns_to_group)[columns_to_average].mean().reset_index()\n",
    "\n",
    "print(\"Original shape:\", merged_df.shape)\n",
    "print(\"New shape:\", result_df.shape)\n",
    "\n",
    "# Rename the columns\n",
    "result_df = result_df.rename(columns={col: f'daily_mean_discharge_{col.split(\"_\")[1]}' for col in columns_to_average})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000 random rows from result_df\n",
    "sample_df = result_df.sample(n=1000, random_state=42)\n",
    "\n",
    "# Select columns to plot (adjust as needed)\n",
    "columns_to_plot = [col for col in sample_df.columns if col.startswith('daily_mean_discharge')]\n",
    "\n",
    "# Create the pair plot\n",
    "sns.pairplot(sample_df[columns_to_plot], kind=\"scatter\", diag_kind=\"kde\", plot_kws={'alpha': 0.3})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets grab some weather station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?station=0CO&data=p01i&year1=2011&month1=1&day1=1&year2=2023&month2=12&day2=31&tz=America%2FDenver&format=onlycomma&latlon=yes&elev=yes&missing=null&trace=0.0001&direct=no&report_type=3&report_type=4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_met = pd.read_csv(url)\n",
    "df_met.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'valid' column to datetime format\n",
    "df_met['valid'] = pd.to_datetime(df_met['valid'])\n",
    "\n",
    "# Extract year from 'valid' column and create a new 'year' column\n",
    "df_met['year'] = df_met['valid'].dt.year\n",
    "\n",
    "# Extract day of year from 'valid' column and create a new 'day_of_year' column\n",
    "df_met['day_of_year'] = df_met['valid'].dt.dayofyear\n",
    "\n",
    "# Extract hour from 'valid' column and create a new 'hour_of_day' column\n",
    "df_met['hour_of_day'] = df_met['valid'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_met.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the first DataFrame with p01i, year, day_of_year, and hour_of_day\n",
    "df1 = df_met[['p01i', 'year', 'day_of_year', 'hour_of_day']]\n",
    "\n",
    "print(\"Shape before dropping NaN:\", df1.shape)\n",
    "df1 = df1.dropna(subset=['p01i'])\n",
    "print(\"Shape after dropping NaN:\", df1.shape)\n",
    "\n",
    "# Create the second DataFrame with station, lat, lon, and elevation, and drop duplicates\n",
    "df_metadata = df_met[['station', 'lat', 'lon', 'elevation']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the original size\n",
    "original_size = len(df1)\n",
    "print(f\"Original dataframe size: {original_size} rows\")\n",
    "\n",
    "# 1. Group by year, day_of_year, and hour_of_day, then calculate mean and sum\n",
    "grouped = df1.groupby(['year', 'day_of_year', 'hour_of_day'])\n",
    "mean_values = grouped['p01i'].mean().reset_index(name='p01i_mean')\n",
    "sum_values = grouped['p01i'].sum().reset_index(name='p01i_sum')\n",
    "\n",
    "# 2. Merge the mean and sum values back to the original dataframe\n",
    "result = df1.merge(mean_values, on=['year', 'day_of_year', 'hour_of_day'])\n",
    "result = result.merge(sum_values, on=['year', 'day_of_year', 'hour_of_day'])\n",
    "\n",
    "# 3. Drop the original 'p01i' column\n",
    "result = result.drop(columns=['p01i'])\n",
    "\n",
    "# 4. Remove duplicates based on year, day_of_year, and hour_of_day\n",
    "result = result.drop_duplicates(subset=['year', 'day_of_year', 'hour_of_day'])\n",
    "\n",
    "# Print the new size and calculate percentage decrease\n",
    "new_size = len(result)\n",
    "print(f\"New dataframe size: {new_size} rows\")\n",
    "\n",
    "decrease = original_size - new_size\n",
    "percentage_decrease = (decrease / original_size) * 100\n",
    "\n",
    "print(f\"Decrease in rows: {decrease}\")\n",
    "print(f\"Percentage decrease: {percentage_decrease:.2f}%\")\n",
    "\n",
    "result_met = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataframes are named df1 and df2\n",
    "result = pd.merge(result_met, result_df, on=['year', 'day_of_year', 'hour_of_day'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.dropna(subset=['daily_mean_discharge_06719505']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataframe can be exported here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting up the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of years\n",
    "all_years = list(range(2011, 2024))  # 2011 to 2023 inclusive\n",
    "\n",
    "# Split the years into train, validation, and test\n",
    "train_years = all_years[:9]  # 2011 to 2019\n",
    "val_years = all_years[9:11]  # 2020 to 2021\n",
    "test_years = all_years[11:]  # 2022 to 2023\n",
    "\n",
    "# Print the year ranges for verification\n",
    "print(\"Train years:\", train_years)\n",
    "print(\"Validation years:\", val_years)\n",
    "print(\"Test years:\", test_years)\n",
    "\n",
    "# Target variable\n",
    "target_col = 'daily_mean_discharge_06719505'\n",
    "\n",
    "# Using 'result' as the dataframe name\n",
    "# Split the data based on the year lists\n",
    "X_train = result[result['year'].isin(train_years)].drop(columns=[target_col])\n",
    "y_train = result[result['year'].isin(train_years)][target_col]\n",
    "\n",
    "X_val = result[result['year'].isin(val_years)].drop(columns=[target_col])\n",
    "y_val = result[result['year'].isin(val_years)][target_col]\n",
    "\n",
    "X_test = result[result['year'].isin(test_years)].drop(columns=[target_col])\n",
    "y_test = result[result['year'].isin(test_years)][target_col]\n",
    "\n",
    "# Drop the 'year' column from X_train, X_val, and X_test\n",
    "X_train = X_train.drop(columns=['year'])\n",
    "X_val = X_val.drop(columns=['year'])\n",
    "X_test = X_test.drop(columns=['year'])\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Print the column names to verify 'year' has been dropped\n",
    "print(\"\\nColumns in X_train:\", X_train.columns.tolist())\n",
    "\n",
    "result = result.astype({col: float for col in result.select_dtypes(include=['int32']).columns})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_all_float64 = all(result.dtypes == np.float64)\n",
    "\n",
    "message = (\n",
    "    \"All columns in the DataFrame are dtype float64.\"\n",
    "    if is_all_float64\n",
    "    else \"Not all columns in the DataFrame are dtype float64.\"\n",
    ")\n",
    "\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the XGBoost model\n",
    "model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# Fit the model with early stopping\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "# Calculate R2\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Training Results:\")\n",
    "print(f\"RMSE: {rmse_train:.4f}\")\n",
    "print(f\"R2: {r2_train:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"RMSE: {rmse_val:.4f}\")\n",
    "print(f\"R2: {r2_val:.4f}\")\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"RMSE: {rmse_test:.4f}\")\n",
    "print(f\"R2: {r2_test:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "importance = model.feature_importances_\n",
    "feature_importance = sorted(zip(importance, X_train.columns), reverse=True)\n",
    "print(\"\\nTop Features:\")\n",
    "for score, feature in feature_importance:\n",
    "    print(f\"{feature}, {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_scatter(y_true, y_pred, title, ax):\n",
    "    ax.scatter(y_true, y_pred, alpha=0.2)\n",
    "    \n",
    "    # Add 1-to-1 line\n",
    "    ax_min = min(ax.get_xlim()[0], ax.get_ylim()[0])\n",
    "    ax_max = max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "    ax.plot([ax_min, ax_max], [ax_min, ax_max], 'r--', lw=2)\n",
    "    \n",
    "    ax.set_xlabel('True Values')\n",
    "    ax.set_ylabel('Predicted Values')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Add R-squared value to the plot\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    ax.text(0.05, 0.95, f'R² = {r2:.4f}', transform=ax.transAxes, \n",
    "            verticalalignment='top')\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot for training data\n",
    "plot_prediction_scatter(y_train, y_train_pred, 'Training Set', ax1)\n",
    "\n",
    "# Plot for validation data\n",
    "plot_prediction_scatter(y_val, y_val_pred, 'Validation Set', ax2)\n",
    "\n",
    "# Plot for test data\n",
    "plot_prediction_scatter(y_test, y_test_pred, 'Test Set', ax3)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_hexbin(y_true, y_pred, title, ax):\n",
    "    hb = ax.hexbin(y_true, y_pred, gridsize=9, cmap='viridis')\n",
    "    \n",
    "    # Set vmax to 1/3 of the actual maximum\n",
    "    vmax = hb.get_array().max() / 3\n",
    "    hb.set_clim(vmax=vmax)\n",
    "    \n",
    "    # Add 1-to-1 line\n",
    "    ax_min = min(ax.get_xlim()[0], ax.get_ylim()[0])\n",
    "    ax_max = max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "    ax.plot([ax_min, ax_max], [ax_min, ax_max], 'r--', lw=2)\n",
    "    \n",
    "    ax.set_xlabel('True Values')\n",
    "    ax.set_ylabel('Predicted Values')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Add R-squared value to the plot\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    ax.text(0.05, 0.95, f'R² = {r2:.4f}', transform=ax.transAxes, \n",
    "            verticalalignment='top', color='white')\n",
    "    \n",
    "    return hb\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot for training data\n",
    "hb1 = plot_prediction_hexbin(y_train, y_train_pred, 'Training Set', ax1)\n",
    "fig.colorbar(hb1, ax=ax1, label='Count', extend='max')\n",
    "\n",
    "# Plot for validation data\n",
    "hb2 = plot_prediction_hexbin(y_val, y_val_pred, 'Validation Set', ax2)\n",
    "fig.colorbar(hb2, ax=ax2, label='Count', extend='max')\n",
    "\n",
    "# Plot for test data\n",
    "hb3 = plot_prediction_hexbin(y_test, y_test_pred, 'Test Set', ax3)\n",
    "fig.colorbar(hb3, ax=ax3, label='Count', extend='max')\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 50  # Increased number of runs\n",
    "all_importances = []\n",
    "all_gain_importances = []\n",
    "all_cover_importances = []\n",
    "\n",
    "# Define a range for each hyperparameter\n",
    "n_estimators_range = range(50, 150)\n",
    "max_depth_range = range(3, 8)\n",
    "learning_rate_range = [0.01, 0.05, 0.1, 0.2]\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"\\nRun {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Bootstrap sampling\n",
    "    X_train_boot, y_train_boot = resample(X_train, y_train, n_samples=len(X_train))\n",
    "    \n",
    "    # Randomly select hyperparameters\n",
    "    n_estimators = np.random.choice(n_estimators_range)\n",
    "    max_depth = np.random.choice(max_depth_range)\n",
    "    learning_rate = np.random.choice(learning_rate_range)\n",
    "    \n",
    "    # Create and train the XGBoost model\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=np.random.randint(1000)  # Random state for each run\n",
    "    )\n",
    "\n",
    "    # Fit the model with early stopping\n",
    "    model.fit(\n",
    "        X_train_boot, y_train_boot,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate RMSE and R2 for test set\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"Test RMSE: {rmse_test:.4f}, R2: {r2_test:.4f}\")\n",
    "\n",
    "    # Store different types of feature importances\n",
    "    all_importances.append(model.feature_importances_)\n",
    "    all_gain_importances.append(model.get_booster().get_score(importance_type='gain'))\n",
    "    all_cover_importances.append(model.get_booster().get_score(importance_type='cover'))\n",
    "\n",
    "# Convert to DataFrames\n",
    "importance_df = pd.DataFrame(all_importances, columns=X_train.columns)\n",
    "gain_df = pd.DataFrame(all_gain_importances).fillna(0)\n",
    "cover_df = pd.DataFrame(all_cover_importances).fillna(0)\n",
    "\n",
    "# Function to create and show boxplot\n",
    "def plot_importance(df, title):\n",
    "    median_importance = df.median().sort_values(ascending=False)\n",
    "    sorted_features = median_importance.index\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    df[sorted_features].boxplot(vert=False, figsize=(12, 8))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each importance type\n",
    "plot_importance(importance_df, 'Feature Importance Distribution (weight)')\n",
    "plot_importance(gain_df, 'Feature Importance Distribution (gain)')\n",
    "plot_importance(cover_df, 'Feature Importance Distribution (cover)')\n",
    "\n",
    "# Print median feature importance for each type\n",
    "for df, imp_type in [(importance_df, \"Weight\"), (gain_df, \"Gain\"), (cover_df, \"Cover\")]:\n",
    "    print(f\"\\nMedian Feature Importance ({imp_type}):\")\n",
    "    median_importance = df.median().sort_values(ascending=False)\n",
    "    for feature, importance in median_importance.items():\n",
    "        print(f\"{feature}: {importance:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
