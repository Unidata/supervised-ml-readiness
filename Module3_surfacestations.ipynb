{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "from io import StringIO\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Union, Dict, List\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://coagmet.colostate.edu/data/nw/hourly.csv?header=yes&from=2018-01-01&to=2023-12-31&dateFmt=iso&tz=co&fields=t,rh,dewpt,windSpeed,windDir,gustSpeed&stations=bld01,bld02,bru01,btd01,crk01,eat01,ftc02,ftc04,ftl02,ftm02,gil01,gly05,jcn01,lmt01,lmt02,lov01,ovd01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the first row\n",
    "df = df.iloc[1:]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_change = ['Air Temp', 'RH', 'Dewpoint', 'Wind', 'Wind Dir', 'Gust Speed']\n",
    "df.loc[:, columns_to_change] = df[columns_to_change].astype(float, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of NaN values in each column\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace multiple values with np.nan\n",
    "df = df.replace([-999.0,], np.nan)\n",
    "\n",
    "# Check the number of NaN values in each column\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of NaN values in each column\n",
    "nan_percentage = (df.isna().sum() / len(df)) * 100\n",
    "\n",
    "# Optionally, format the output to 2 decimal places\n",
    "print(\"\\nPercentage of NaN values in each column (formatted):\")\n",
    "print(nan_percentage.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_stations = df['Station'].nunique()\n",
    "\n",
    "print(f\"Number of unique stations: {num_unique_stations}\")\n",
    "\n",
    "unique_stations_list = df['Station'].unique().tolist()\n",
    "\n",
    "print(f\"List of unique stations: {unique_stations_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime\n",
    "df['date'] = pd.to_datetime(df['Date and Time'])\n",
    "\n",
    "# Find the lowest date\n",
    "lowest_date = df['date'].min()\n",
    "\n",
    "# Create a new column 'day_index' starting from 0\n",
    "df['day_index'] = (df['date'] - lowest_date).dt.days\n",
    "\n",
    "# Create a new column 'hour_index' going from 0 to 23\n",
    "df['hour_index'] = df['date'].dt.hour\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_stations = random.sample(unique_stations_list, 3)\n",
    "print(f\"Randomly selected stations: {selected_stations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound = 1460\n",
    "lower_bound = upper_bound-365\n",
    "\n",
    "df_filtered = df[(df['day_index'] >= lower_bound) & (df['day_index'] <= upper_bound)]\n",
    "\n",
    "# Filter the DataFrame for the lower range (new DataFrame)\n",
    "df_lower_range = df[(df['day_index'] >= 0) & (df['day_index'] < lower_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lower_range[df_lower_range['Station'] == 'lmt01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_station_data(df, station):\n",
    "    print(f\"Processing station: {station}\")\n",
    "    print(f\"Shape of df before filtering: {df.shape}\")\n",
    "    \n",
    "    # Filter the DataFrame for the specific station\n",
    "    df_station = df[df['Station'] == station].copy()\n",
    "    \n",
    "    print(f\"Shape of df_station after filtering: {df_station.shape}\")\n",
    "    \n",
    "    if df_station.empty:\n",
    "        print(f\"No data found for station {station}\")\n",
    "        return None\n",
    "    \n",
    "    # Append station name to specific columns\n",
    "    columns_to_rename = ['Air Temp', 'RH', 'Dewpoint', 'Wind']\n",
    "    for col in columns_to_rename:\n",
    "        if col in df_station.columns:\n",
    "            df_station.rename(columns={col: f'{col}_{station}'}, inplace=True)\n",
    "    \n",
    "    return df_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Station', 'Air Temp', 'RH', 'Wind', 'Dewpoint', 'day_index', 'hour_index']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging: Print information about df and unique_stations_list\n",
    "print(f\"Shape of df: {df.shape}\")\n",
    "print(f\"Columns in df: {df.columns}\")\n",
    "print(f\"Unique values in 'Station' column: {df['Station'].unique()}\")\n",
    "print(f\"unique_stations_list: {unique_stations_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store the merged results\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each station in the list of unique stations\n",
    "for station in unique_stations_list:\n",
    "    # Process the data for the current station\n",
    "    processed_df = process_station_data(df, station)\n",
    "    \n",
    "    # Check if the processed data is valid (not None and not empty)\n",
    "    if processed_df is not None and not processed_df.empty:\n",
    "        # Drop the station name column from the processed DataFrame\n",
    "        processed_df = processed_df.drop(columns=['Station'])  # Adjust 'station_name' to match your actual column name\n",
    "        \n",
    "        # If merged_df is empty, this is the first valid data we've processed\n",
    "        if merged_df.empty:\n",
    "            merged_df = processed_df\n",
    "        else:\n",
    "            # If merged_df already contains data, concatenate the new data vertically\n",
    "            merged_df = pd.merge(merged_df, processed_df, \n",
    "                     on=['day_index', 'hour_index'], \n",
    "                     how='outer')\n",
    "        \n",
    "        # Print a success message for this station\n",
    "        print(f\"Station {station} processed successfully!\")\n",
    "    else:\n",
    "        # If processed_df is None or empty, print a message and skip this station\n",
    "        print(f\"Skipping empty or None result for station {station}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is called 'merged_df'\n",
    "# First, let's select only the 'Air Temp' columns\n",
    "air_temp_columns = [col for col in merged_df.columns if col.startswith('Air Temp')]\n",
    "\n",
    "# Create a new DataFrame with only these columns\n",
    "air_temp_df = merged_df[air_temp_columns]\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(20, 20))  # Adjust the size as needed\n",
    "\n",
    "# Create the pair plot\n",
    "sns.pairplot(air_temp_df.sample(1000), diag_kind='kde', plot_kws={'alpha': 0.1})\n",
    "\n",
    "# Add a title\n",
    "plt.suptitle('Pair Plot of Air Temperature Across All Stations', y=1.02, fontsize=16)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is called 'merged_df'\n",
    "# Select only the 'RH' columns\n",
    "rh_columns = [col for col in merged_df.columns if col.startswith('RH')]\n",
    "\n",
    "# Create a new DataFrame with only these columns\n",
    "rh_df = merged_df[rh_columns]\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(20, 20))  # Adjust the size as needed\n",
    "\n",
    "# Create the pair plot\n",
    "sns.pairplot(rh_df.sample(1000), diag_kind='kde', plot_kws={'alpha': 0.3})\n",
    "\n",
    "# Add a title\n",
    "plt.suptitle('Pair Plot of Relative Humidity Across All Stations', y=1.02, fontsize=16)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is called 'merged_df'\n",
    "# Select only the 'Wind' columns\n",
    "wind_columns = [col for col in merged_df.columns if col.startswith('Wind')]\n",
    "\n",
    "# Create a new DataFrame with only these columns\n",
    "wind_df = merged_df[wind_columns]\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(20, 20))  # Adjust the size as needed\n",
    "\n",
    "# Create the pair plot\n",
    "sns.pairplot(wind_df, diag_kind='kde', plot_kws={'alpha': 0.1})\n",
    "\n",
    "# Add a title\n",
    "plt.suptitle('Pair Plot of Wind Speed Across All Stations', y=1.02, fontsize=16)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_rolling_average_and_median(df, window_size=5):\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    for column in df_imputed.columns:\n",
    "        if df_imputed[column].dtype.kind in 'biufc':  # Check if column is numeric\n",
    "            # Create a Series with the rolling mean\n",
    "            rolling_mean = df_imputed[column].rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "            \n",
    "            # Use the rolling mean to fill NaN values\n",
    "            df_imputed[column] = df_imputed[column].fillna(rolling_mean)\n",
    "            \n",
    "            # If any NaNs remain, fill with the median of the column\n",
    "            if df_imputed[column].isna().any():\n",
    "                column_median = df_imputed[column].median()\n",
    "                df_imputed[column] = df_imputed[column].fillna(column_median)\n",
    "                print(f\"Column '{column}': Filled remaining NaNs with median ({column_median})\")\n",
    "        else:\n",
    "            print(f\"Column '{column}' is non-numeric. Skipping imputation.\")\n",
    "    \n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = impute_with_rolling_average_and_median(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_imputed.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your list of target column suffixes\n",
    "target_suffixes = ['ovd01', 'bru01']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of days for each split\n",
    "total_days = df_imputed['day_index'].nunique()\n",
    "train_days = int(total_days * 0.80)\n",
    "test_days = int(total_days * 0.15)\n",
    "validation_days = total_days - train_days - test_days  # This ensures we use all the data\n",
    "\n",
    "# Get the day_index values for split points\n",
    "train_end = df_imputed['day_index'].unique()[train_days - 1]\n",
    "test_end = df_imputed['day_index'].unique()[train_days + test_days - 1]\n",
    "\n",
    "# Split the data\n",
    "train_df = df_imputed[df_imputed['day_index'] <= train_end]\n",
    "test_df = df_imputed[(df_imputed['day_index'] > train_end) & (df_imputed['day_index'] <= test_end)]\n",
    "validation_df = df_imputed[df_imputed['day_index'] > test_end]\n",
    "\n",
    "# Print the sizes of each split to verify\n",
    "print(f\"Train set: {len(train_df)} rows ({len(train_df)/len(df_imputed)*100:.2f}%)\")\n",
    "print(f\"Test set: {len(test_df)} rows ({len(test_df)/len(df_imputed)*100:.2f}%)\")\n",
    "print(f\"Validation set: {len(validation_df)} rows ({len(validation_df)/len(df_imputed)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(df, suffixes):\n",
    "    # Create the regex pattern from the list of suffixes\n",
    "    pattern = '|'.join(f'{suffix}$' for suffix in suffixes)\n",
    "    return df.filter(regex=pattern)\n",
    "\n",
    "\n",
    "# Filter target columns\n",
    "target_columns = filter_columns(train_df, target_suffixes).columns\n",
    "\n",
    "# Create X_train and y_train\n",
    "df_X_train = train_df.drop(columns=target_columns)\n",
    "df_X_train = df_X_train.select_dtypes(include=['float64'])\n",
    "df_y_train = filter_columns(train_df, target_suffixes)\n",
    "\n",
    "# Create X_test and y_test\n",
    "df_X_test = test_df.drop(columns=target_columns)\n",
    "df_X_test = df_X_test.select_dtypes(include=['float64'])\n",
    "df_y_test = filter_columns(test_df, target_suffixes)\n",
    "\n",
    "# Create X_validation and y_validation\n",
    "df_X_validation = validation_df.drop(columns=target_columns)\n",
    "df_X_validation = df_X_validation.select_dtypes(include=['float64'])\n",
    "df_y_validation = filter_columns(validation_df, target_suffixes)\n",
    "\n",
    "# Verify the shapes of the resulting dataframes\n",
    "print(\"X_train shape:\", df_X_train.shape)\n",
    "print(\"y_train shape:\", df_y_train.shape)\n",
    "print(\"X_test shape:\", df_X_test.shape)\n",
    "print(\"y_test shape:\", df_y_test.shape)\n",
    "print(\"X_validation shape:\", df_X_validation.shape)\n",
    "print(\"y_validation shape:\", df_y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare the data\n",
    "X_train = df_X_train\n",
    "y_train = df_y_train\n",
    "X_test = df_X_test\n",
    "y_test = df_y_test\n",
    "X_validation = df_X_validation\n",
    "y_validation = df_y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost model\n",
    "xgb_model = MultiOutputRegressor(xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')\n",
    "rmse = np.sqrt(mse)  # Calculate RMSE\n",
    "r2 = r2_score(y_test, y_pred, multioutput='raw_values')\n",
    "\n",
    "# Print results\n",
    "print(\"Mean Squared Error for each target:\")\n",
    "for target, error in zip(y_test.columns, mse):\n",
    "    print(f\"{target}: {error:.4f}\")\n",
    "\n",
    "print(\"\\nRoot Mean Squared Error for each target:\")\n",
    "for target, error in zip(y_test.columns, rmse):\n",
    "    print(f\"{target}: {error:.4f}\")\n",
    "\n",
    "print(\"\\nR2 Score for each target:\")\n",
    "for target, score in zip(y_test.columns, r2):\n",
    "    print(f\"{target}: {score:.4f}\")\n",
    "\n",
    "# Overall performance\n",
    "print(f\"\\nAverage MSE: {np.mean(mse):.2f}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse):.2f}\")\n",
    "print(f\"Average R2 Score: {np.mean(r2):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_validation)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_validation, y_pred, multioutput='raw_values')\n",
    "rmse = np.sqrt(mse)  # Calculate RMSE\n",
    "r2 = r2_score(y_validation, y_pred, multioutput='raw_values')\n",
    "\n",
    "# Print results\n",
    "print(\"Mean Squared Error for each target:\")\n",
    "for target, error in zip(y_validation.columns, mse):\n",
    "    print(f\"{target}: {error:.4f}\")\n",
    "\n",
    "print(\"\\nRoot Mean Squared Error for each target:\")\n",
    "for target, error in zip(y_validation.columns, rmse):\n",
    "    print(f\"{target}: {error:.4f}\")\n",
    "\n",
    "print(\"\\nR2 Score for each target:\")\n",
    "for target, score in zip(y_validation.columns, r2):\n",
    "    print(f\"{target}: {score:.4f}\")\n",
    "\n",
    "# Overall performance\n",
    "print(f\"\\nAverage MSE: {np.mean(mse):.2f}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse):.2f}\")\n",
    "print(f\"Average R2 Score: {np.mean(r2):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
